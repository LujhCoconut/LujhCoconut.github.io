# DistServe [OSDI'24]

【Title】DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving



* 在大语言模型（LLM）应用中，延迟（latency）是一个关键的性能指标，尤其是在模型处理请求的两个主要阶段中：

  * **Prefill Phase（预填充阶段）**：这是模型接收输入并开始生成第一个输出（例如，第一个文本token）的阶段。延迟的指标是**Time to First Token (TTFT)**，即从用户发送请求到模型生成第一个token所需的时间。TTFT通常受到模型大小、输入长度和计算能力的影响。

  * **Decoding Phase（解码阶段）**：这是模型逐步生成其余输出token的阶段。在这个过程中，每个token的生成都需要一定的时间。延迟的指标是**Time Per Output Token (TPOT)**，即每生成一个token所需的时间。TPOT通常反映了模型生成速度的效率。



* 当应用程序对延迟有严格要求时，现有系统通常面临以下两种选择：
  * **优先满足某一阶段的延迟需求**：例如，在生成文本的过程中，如果用户更关心第一个输出的速度（Time to First Token, TTFT），系统可能会优化预填充阶段，牺牲生成后续token的速度（Time Per Output Token, TPOT）。反之，如果后续输出的流畅性更重要，系统可能会优化解码阶段。
  * **增加计算资源以同时满足两个阶段的延迟需求**：如果既要快速生成第一个token，又要保证后续输出的生成速度，系统可能需要投入更多的计算资源（如更强大的硬件、更大的并行计算能力）来同时优化TTFT和TPOT。这种方式虽然有效，但代价高昂，不一定在所有场景下都可行。



## Introduction

大型语言模型（LLMs），如 GPT-4、Bard 和 LLaMA，代表了生成式人工智能领域的重大突破。它们正在重塑现有的互联网服务，从搜索引擎到个人助手，并催生出全新的应用，例如通用聊天机器人和编程助手。然而，这些进步也带来了显著的挑战：**处理端到端的 LLM 查询比标准搜索查询的速度要慢得多**。为了满足各种应用对严格延迟的要求，服务提供商需要过度配置计算资源，尤其是大量 GPU，这导致了成本效率的降低。**因此，在确保高服务水平目标（SLO）达成率的同时，优化每次 LLM 查询的成本，正成为所有 LLM 服务的关键任务。**

> SLO 的全称是 **Service Level Objective**，即 **服务水平目标**。
>
> 这是服务级别协议（SLA，Service Level Agreement）中的核心组成部分，用于定义服务提供商在服务质量方面的具体目标和衡量标准。通常包括对服务性能、可用性、延迟等的具体要求。例如：
>
> - **响应时间**：99%的请求需要在200毫秒内完成。
> - **可用性**：服务需要达到99.9%的正常运行时间。
>
> SLO 的设定对服务的运营和用户体验至关重要，同时也影响资源分配和成本优化策略。

LLM 服务在响应用户查询时分为两个阶段。

* **预填充阶段（prefill phase）处理用户的提示（由一系列 token 组成），并在一步内生成响应的第一个 token。**
* 随后是**解码阶段（decoding phase）**，该阶段逐步生成后续的 token；每次解码步骤根据之前生成的 token 生成一个新的 token，直到生成终止 token。

这种两阶段流程使 LLM 服务区别于传统服务——LLM 服务的延迟通过两个关键指标来衡量：

* **第一个 token 的生成时间（Time to First Token, TTFT）**，即预填充阶段的持续时间，
* **每个输出 token 的生成时间（Time Per Output Token, TPOT）**，表示除第一个 token 外，每次请求生成一个 token 所需的平均时间。

不同的应用对这两个指标的需求各不相同。

> 例如
>
> * 实时聊天机器人优先考虑低 TTFT 以提高响应速度，而只要 TPOT 快于人类阅读速度（约 250 词/分钟），其重要性相对较低。
> * 相反，文档摘要生成则更关注低 TPOT，以加快摘要生成速度



因此，鉴于应用对 TTFT 和 TPOT 的需求，一个高效的 LLM 服务系统应平衡这些需求，并最大化每个 GPU 的**有效吞吐量（goodput）**。有效吞吐量被定义为在满足服务水平目标（SLO，例如 90% 达成率）的前提下，每个 GPU 可处理的最大请求率——更高的每 GPU 有效吞吐量直接转化为更低的每次查询成本。

PD两阶段共享 LLM 的权重和工作内存，现有的LLM将两阶段**置于同一GPU，对请求进行批处理**来最大化整体系统吞吐量，即所有用户和请求的每秒生成 token 数量。然而，为了满足延迟要求，这些系统往往需要过度配置计算资源。

![](.\distserve_p1.png)

如图所示，现有的系统同时将PD两阶段部署在一个GPU上，会使得有效吞吐量比在单GPU上单独P或者单独D，小了两倍左右。这一有效吞吐量的差距主要来源于预填充和解码阶段的**合并部署**，而这两个阶段具有截然不同的计算特性和延迟需求。

* 首先，共同部署会导致**预填充阶段与解码阶段的严重干扰**。
  * 预填充步骤通常比解码步骤耗时更长。当二者被批处理在一起时，批处理中解码步骤会因预填充步骤的延迟而受到影响，显著增加其 TPOT（每输出 token 的时间）；
  * 同样地，解码步骤的引入也会导致 TTFT（首次 token 的时间）非小幅度地增长，如图 2 所示。

* 即使将两者分开调度，问题依然存在，因为它们会开始争夺资源。
  * 等待 GPU 执行的解码任务会因正在进行的预填充任务而面临更长的排队延迟，反之亦然。如果优先调度其中一个阶段，可能会导致无法满足另一个阶段的延迟要求。



* 预填充阶段和解码阶段在延迟需求以及对不同形式并行化的偏好上存在差异（§3）。然而，将预填充和解码共同部署会使它们的资源分配绑定在一起，无法针对每个阶段的特定延迟需求实施更适合的并行化策略。



为了解决这些挑战，我们提出将 LLM 推理中的预填充阶段和解码阶段分离（P/D分离），分别分配到不同的 GPU 上。这一方法具有两个优势：

* 首先，在不同 GPU 上独立运行每个阶段可以消除预填充与解码的干扰。
* 其次，它能够独立扩展每个阶段，根据其具体的延迟需求，实施量身定制的资源分配和模型并行化策略。

尽管分离可能导致 GPU 之间需要传输中间状态，但我们证明，在现代 GPU 集群中，这种通信开销微乎其微（§3.3）。通过适当管理，分离策略能够显著提升每 GPU 的吞吐量。



基于上述见解，本工作构建了 **DistServe**，一个通过分离预填充阶段和解码阶段优化吞吐量的 LLM 服务系统。

* 针对 TTFT 和 TPOT 的要求，DistServe 首先通过联合优化预填充和解码阶段的 GPU 分配及并行策略，在假设服务单个模型副本的情况下，独立扩展每个阶段。该优化过程确保了每 GPU 吞吐量的最大化，并根据各自的延迟需求为每个阶段分配不同数量的 GPU 和并行化策略。

* 随后，DistServe 通过复制这种分配方案扩展至多个实例，以满足用户要求的流量需求（§4）。

* 此外，DistServe 还提供了一种算法，根据分配方案和集群带宽，在预填充和解码计算之间合理放置任务，尽可能减少阶段之间传输中间状态的开销。

  

我们将 DistServe 实现为 **LLM 推理引擎之上的一个协调层**。我们在多种 LLM 上评估了 DistServe，并基于三种重要的现实世界 LLM 应用（聊天机器人、编程助手和文档摘要）变换工作负载。与现有的最先进解决方案相比，DistServe 在不同的延迟约束下可以**提供最多 7.4 倍的请求吞吐量或 12.6 倍更严格的 SLO**。我们的贡献包括：

* 识别现有 LLM 服务系统中预填充-解码干扰和资源耦合的问题，并提出将这两个阶段分离的方法。
* 设计一种新颖的任务调度算法，自动选择预填充和解码实例的最优吞吐量方案。
* 对 DistServe 进行全面评估，使用真实工作负载进行测试。



## Background and Motivation

LLM 服务遵循**客户端-服务器（B/S）架构**

* 客户端将一段文本作为请求提交给服务器；
* 服务器将 LLM 部署在 GPU 上，执行请求的推理，并将生成的结果（或流式数据）返回给客户端。

如`Introduction`所述，由于独特的预填充-解码（P/D）过程，LLM 服务可能会对 TTFT 和 TPOT 设置严格的服务级目标（SLO），这些目标根据应用的需求有所不同。服务系统必须同时满足这两个 SLO，同时最小化与昂贵 GPU 相关的成本。换句话说，我们希望**服务系统能够最大化每秒处理的请求数量**，同时确保每个 GPU 的 SLO 达成目标——即**最大化每 GPU 的吞吐量**。接下来，我们将详细介绍 LLM 推理计算，并讨论现有的 LLM 服务优化方法。



### LLM Inference

现代 LLMs 预测给定输入序列的下一个 token。这个预测过程涉及计算序列中每个 token 的隐表示（`hidden representation`）。LLM 可以接受可变数量的输入 tokens，并并行计算它们的隐表示，其计算工作负载随着并行处理的 token 数量的增加呈超线性增长。**无论输入 token 的数量如何，计算都需要大量的 I/O 操作，将 LLM 权重和中间状态从 GPU 的 HBM 移动到 SRAM。**这一过程在不同的输入大小下是一致的。

> "Hidden representation"（隐表示）是指在神经网络，尤其是在深度学习模型（如大型语言模型，LLM）中，每一层通过输入数据进行计算后产生的中间输出。这个表示通常是对输入数据的一种抽象表达，包含了数据的关键特征，帮助模型理解和处理信息。

预填充`(prefill)`步骤处理一个新的序列，这个序列通常包含多个标记，并且这些标记是并行处理的。与预填充不同，每个解码步骤仅处理前一步生成的一个新标记。这导致了两个阶段之间的显著计算差异。在处理较长的用户提示时，预填充步骤通常是计算密集型的。例如，对于一个13B的LLM，计算一个512标记序列的预填充使得A100几乎达到计算瓶颈（参见§3.1）。相比之下，尽管每个步骤仅处理一个新标记，解码阶段却与预填充阶段在I/O方面具有相似的需求，使其受到GPU内存带宽的限制。

> 总结一下这句话：`prefill`是计算密集型，`decode`是I/O密集型

在P/D两个阶段中，称为KV Cache的中间状态在每个标记位置生成，并在后续的解码步骤中再次使用。为了避免重新计算，它们被保存在GPU内存中。由于LLM权重和KV Cache在内存中被共享使用，大多数LLM推理引擎选择将预填充和解码阶段放在同一个GPU上，尽管这两个阶段具有不同的计算特性。



### LLM Serving Optimization

在实时在线服务中，多个请求同时到来，必须在SLO（服务水平目标）内处理。**批处理**和**并行化**它们的计算是实现低延迟、高吞吐量和高GPU利用率的关键。

#### **Batching**

当前的服务系统使用了一种称为连续批处理的批处理技术。该方法将新请求的预填充与正在进行的请求的解码进行批处理。它提高了GPU的利用率，并最大化了整个系统的吞吐量——即所有用户和请求每秒生成的令牌。然而，如前所述，这种方法在TTFT和TPOT之间存在权衡。一种先进的连续批处理变体试图通过将长预填充分割成小块并将解码任务与分块的预填充结合来平衡TTFT和TPOT——但本质上，它是**用TTFT换取TPOT**，并不能消除干扰。总之，批处理预填充和解码不可避免地在TTFT或TPOT之间做出妥协。

#### Model parallelism

在LLM服务中，模型并行性通常分为操作内并行性和操作间并行性(`intra-` and `inter-`)。两者都可以用来支持更大的模型，但可能对服务性能的影响不同。

* 操作内并行性(`intra-`)将计算密集型操作（如矩阵乘法）分割到多个GPU上，从而**加速计算，但会导致大量通信**。它**减少了执行时间**，因此也减少了延迟，尤其是对预填充阶段的TTFT，但**需要高带宽的GPU间连接**（例如，NVLINK）。
* 操作间并行性(`inter-`)将LLM层组织成阶段，每个阶段在一个GPU上运行，从而形成流水线。由于阶段间的通信，它适度增加了执行时间，但**随着每个新增GPU的加入，系统的处理能力线性扩展**。
* 在本文中，我们揭示了模型并行性的另一个好处：减少预填充和解码阶段的排队延迟，源于更短的执行时间。除了模型并行性，复制模型实例，无论其模型并行性配置如何，都会线性地扩展系统的处理能力。

> 这些并行策略创建了一个复杂的优化空间，需要根据应用的延迟要求进行仔细的权衡。

#### Problems and Opportunities

将预填充和解码计算放在一起并进行批处理以最大化整个系统的吞吐量，如现有系统所做的那样，对于服务提供商来说是具有成本效益的。然而，在存在服务级别目标（SLOs）的情况下，现有的方法由于以下讨论的问题，难以在保持**高服务质量和低成本**之间找到平衡。

**Prefill-decoding interference**

如图2所示，将一个预填充任务添加到一批解码请求中会显著减慢两个过程，导致TTFT和TPOT的显著增加。

* 具体而言，批处理中的解码任务必须等待较长时间的预填充任务完成，从而延长了TPOT；
* 随着预填充时间的增加，减速效应会加剧，如图2(b)所示。将解码任务添加到预填充过程中还会增加完成预填充任务的时间，特别是在GPU已经达到最大负载时（图2的蓝色曲线）。

![](.\distserve_p2.png)

