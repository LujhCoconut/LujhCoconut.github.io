# Lock

## 乐观与悲观

**Q1: 乐观锁和悲观锁的优缺点以及适用场景是什么？操作系统中有哪些场景使用了乐观锁或者悲观锁？**

* **乐观锁**：
  * 优点：
    * 性能较高：无锁状态下读操作不会阻塞。
    * 适用于读多写少（低冲突）的应用场景，因为更新冲突几率较小。
  * 缺点：
    * 存在写操作失败的可能，例如在高冲突场景下频繁回滚或重试会降低性能。
  * 适用场景：
    * 数据库事务场景，例如基于版本号的并发控制。
    * 微服务中，更新资源时采用的非阻塞更新模型。
  * **操作系统中的应用场景**：
    * 操作系统中大多数内核数据结构使用 **RCU（Read-Copy-Update）** 技术：RCU 是一种高效的乐观锁实现，允许读操作无锁地访问共享数据，而写操作以更新为主，例如 Linux 内核的任务调度器读取任务队列时。
* **悲观锁**：
  * 优点：
    * 可以保证数据一致性，在高并发场景下有效防止读写冲突。
  * 缺点：
    * 开销较大，由于锁机制会导致更多的线程等待，降低系统整体吞吐量。
    * 死锁风险，需要小心设计。
  * 适用场景：
    * 银行转账系统中，防止并发事务中获取相同资源（如数据库余额提取时产生错误）。
  * **操作系统中的应用场景**：
    * 在操作系统中，**inode 锁的悲观实现** 会用于读取文件元数据（如大小或修改时间）。
    * 卷管理文件系统（如 XFS）中常用的互斥锁实现文件的元数据更新。

------

## 自旋与互斥

**Q2: 什么是自旋锁？自旋锁相比于互斥锁的优缺点？操作系统中有哪些地方使用了自旋锁？**

[ghostrace]([ghostrace](https://lujhcoconut.github.io/Others/USENIXSec/GhostRace.html))

**自旋锁定义**： 自旋锁是一种非阻塞锁，当线程尝试获取锁失败时不会进入阻塞队列等待，而是持续“自旋”（轮询），直到获取锁成功。

**优缺点**：

* 优点：
  * 避免线程阻塞/唤醒带来的性能开销：适合锁持有时间很短的场景。
  * 实现简单，常用在内核态中断处理等对性能敏感的区域。
* 缺点：
  * 持续占用 CPU 时间：可能降低 CPU 整体性能
  * 不适合锁持有时间较长的场景
* 注意：自旋锁不适合线程切换频繁的场景，因为频繁轮询会浪费大量 CPU 资源
* **操作系统中的自旋锁**
  * **Linux 内核**：自旋锁经常用于防止中断上下文与用户上下文间的竞争。例如，中断处理程序修改全局变量时，可能会使用自旋锁保护数据。
  * 示例：Linux 内核调度器使用自旋锁保护进程切换的 CPU 核心资源，避免多个线程同时调度到同一 CPU。

------

## 可重入锁

**Q3: 什么是可重入锁？在多线程编程中的作用是什么？操作系统中是否支持类似机制？**

**可重入锁定义**： 可重入锁是一种允许同一个线程多次获取的锁，即一个线程在持有锁的情况下可以重复获取而不会导致死锁。内部通过计数器记录锁的获取和释放次数。

**作用**：

* 支持递归调用：一个方法中可能调用另一个加锁的方法（或自己），如果锁不可重入，会发生死锁。
* 增加系统的灵活性：保证复杂调用链中的线程安全性。

**操作系统中的可重入机制**：

* **信号安全函数**：在操作系统中，可重入性用于保证中断信号处理不会破坏主线程的状态。例如，Linux 的特定系统调用（如 `setjmp` 和 `siglongjmp`）是可重入的。
* **Linux 信号量（Semaphore）**：针对多核环境下的资源共享，信号量可以支持可重入机制多个线程计数锁。

------

## 分布式锁

**Q4: 分布式锁在什么情况下会出现问题？如何解决这些问题？**

- **锁超时问题**：

  - 当任务执行时间大于锁的生存时间时，锁会被误删，其他线程可能错误地获取锁。

- **锁丢失问题**：

  - Redis 崩溃或主从同步出现延迟，导致锁失效。

  - ```
    时间 操作
    T1  客户端A从主节点获取锁成功
    T2  主节点崩溃，锁未同步到从节点
    T3  从节点B升级为新主节点
    T4  客户端B从新主节点获取相同锁成功 ← 锁丢失！
    T5  客户端A和B同时操作共享资源
    ```

- **死锁问题**：

  - 在分布式环境中，多个服务间交互可能导致循环依赖，引发死锁。

**解决方案**：

- **锁续租（延签）**：
  - 在任务执行中，定期检查当前锁是否快超时，通过心跳机制延长锁的有效期。
    - 启动一个守护线程，定期检查锁是否仍被持有，并在锁即将过期时延长其生存时间。
    - **续租时机**：在锁过期时间的1/3到2/3之间开始续租
    - **续租频率**：一般为过期时间的1/3
    - **失败处理**：续租失败时立即重试，连续失败多次则放弃锁
- **设置唯一标识**：
  - 分布式锁时，使用线程或服务唯一标识（如 UUID）进行识别，避免误解锁。
- **Redlock 算法**：
  - 在多个 Redis 节点中锁实现基于 Quorum 的机制，只有获得大多数节点锁后才算加锁成功。
    - 在N个独立的Redis节点上获取锁，当且仅当获得大多数节点（N/2+1）的锁时，才算成功获取。

------

## 信号量

**Q5: 什么是信号量？与互斥锁的主要区别是什么？可以举例说明吗？**

**信号量**： 信号量（Semaphore）是一种计数器，用于管理对共享资源的访问权限。它允许多个线程同时访问固定数量的资源。

- 基本操作：`wait`（P操作）减少信号量；`signal`（V操作）增加信号量。
- 信号量是线程间协作工具，与互斥锁不同。

**互斥锁对比信号量**：

- **互斥锁**：只能有一个线程获得锁，因此很适合保证独占访问。
- **信号量**：允许多个线程访问资源，适合限制对资源的总访问数。

**关键区别**：

- 互斥锁值只为 `0` 或 `1`。
- 信号量可以包含正整数。

**操作系统中的应用**：

- **信号量**：
  - 操作系统中的线程池，用信号量管理最大线程数。例如，Linux 使用信号量限制读/写访问。
- **互斥锁**：
  - 常用于抢占文件锁、I/O 设备控制权等。

------

## 自旋锁性能陷阱+NUMA+MCS锁+qspinlock

**Q6: 自旋锁的使用中如何避免性能陷阱？在 NUMA 架构（非一致性内存访问）中它会带来什么影响？**

**性能陷阱**：

在自旋锁中，线程会不断占用 CPU 流水线进行轮询等待，这可能导致：

- **高 CPU 开销**：其他任务可能无法获取 CPU 时间片。
- **缓存伪共享**：多个线程频繁更新共享变量（如锁标志）导致同一缓存行被大量无效化，进一步降低性能。
  - 缓存行是CPU与内存之间数据传输的最小单位，通常是**64字节**。如果两个无关的变量在内存中靠得特别近，近到足以被放进同一个64字节的块里，那么它们就被“绑定”在了一起。多核并发修改，激活MESI协议。因为`lock1`和`lock2`在同一个缓存行，当线程1获取`lock1`（将其从0改为1）时，会导致线程2用于自旋检查`lock2`的缓存行失效。线程2必须从内存或核心1重新加载整个缓存行，才能再次检查`lock2`是否可用。这造成了巨大的、完全不必要的性能开销。逻辑上无竞争，性能上却像在竞争最激烈的一把锁。解决方案的核心就是**主动填充内存，让这些变量各自独占一个缓存行**。

**性能优化方法**：

- **自旋时间限制**：
  - 设置自旋次数上限，超过阈值后线程进入阻塞状态（如使用操作系统的 `sched_yield()`）。
- **指数回退（Exponential Backoff）**：
  - 增加线程重试之间的时间间隔。避免多个线程同时争抢锁，产生激烈的竞争。
- **使用 CPU pause 指令**：
  - 在 x86 架构下，使用 `pause` 指令（例如 `__asm__ __volatile__ ("pause" ::: "memory")`）可以减少自旋影响并防止流水线阻塞。

**NUMA 架构中的问题**：

- **伪共享**：
  - 在 NUMA 环境，锁的标志位可能跨越多个 NUMA 节点的缓存，这会导致频繁的远程内存访问，增加缓存一致性协议的成本。
- **优化策略**：
  - 使用 **局部锁（Local Locking）**，即将锁分布到每个 NUMA 节点的本地内存中，减少跨节点访问。
  - 使用更高效的数据结构如 MCS 锁，MCS 锁将队列锁的队列节点分配到线程本地，减少内存共享。
- **操作系统中的应用**：
  - 自旋锁在底层内核中，如 **Linux 内核**，被广泛用于短期锁定操作（如中断处理程序）。但当用于 NUMA 架构时，Linux 内核通过 `qspinlock`（队列自旋锁）解决伪共享问题。



**MCS**

MCS锁（Mellor-Crummey Scott Lock）是一种**基于队列的公平自旋锁**。它的核心理念是：

* **每个线程在自己的本地变量上自旋**，而不是在共享变量上
* 通过链表组织等待线程，保证FIFO公平性
* 完全消除缓存伪共享

```C
// MCS节点，每个线程一个
struct mcs_node {
    struct mcs_node *next;    // 指向下一个等待者
    int locked;               // 1=需要等待，0=可以获取锁
    // 缓存行填充，确保每个节点独占一个缓存行
    char padding[CACHELINE_SIZE - sizeof(struct mcs_node*) - sizeof(int)];
};

// MCS锁
struct mcs_lock {
    struct mcs_node *tail;    // 指向队列尾部
};
```

假设线程T1\T2\T3按顺序请求锁

```
初始状态:
lock->tail = NULL
T1节点: {next=NULL, locked=1}
T2节点: {next=NULL, locked=1}
T3节点: {next=NULL, locked=1}

T1加锁:
1. lock->tail 从 NULL → T1
2. 无前驱，T1直接获取锁

T2加锁:
1. lock->tail 从 T1 → T2, prev=T1
2. T1.next = T2
3. T2在T2.locked上自旋(等待T1释放)

T3加锁:
1. lock->tail 从 T2 → T3, prev=T2
2. T2.next = T3
3. T3在T3.locked上自旋

T1解锁:
1. 发现T1.next=T2不为NULL
2. 设置T2.locked=0
3. T2从自旋中退出，获取锁

T2解锁:
1. 发现T2.next=T3不为NULL
2. 设置T3.locked=0
3. T3从自旋中退出，获取锁
```



**QSpinLock**

QSpinlock是Linux内核中使用的**高效队列自旋锁**。它结合了票据锁和MCS锁的优点：

* 使用单个32位字表示锁状态
* 前24位表示队列尾，后8位表示锁状态
* 向后兼容传统自旋锁接口

```c
// Linux内核中的qspinlock定义
typedef struct qspinlock {
    atomic_t val;  // 32位原子变量
} spinlock_t;

// val的位布局：
// 位  0: locked bit (锁是否被持有)
// 位  1-2: pending bit (优化用)
// 位  3-7: 未使用
// 位  8-31: tail (队列尾部编码)
```

QSpinlock的关键优化是**pending位**，用于处理低竞争场景：

```c
// 简化版qspinlock逻辑
void queued_spin_lock(struct qspinlock *lock) {
    // 1. 首先尝试快速路径：直接获取锁
    if (likely(atomic_try_cmpxchg_acquire(&lock->val, 0, _Q_LOCKED_VAL)))
        return;
    
    // 2. 设置pending位，表示"我在排队"
    atomic_or(&lock->val, _Q_PENDING_VAL);
    
    // 3. 自旋等待
    for (;;) {
        if (val & _Q_LOCKED_VAL) {
            // 锁被持有，继续等待
            cpu_relax();
        } else {
            // 锁可用，尝试获取
            if (atomic_try_cmpxchg(&lock->val, &old, _Q_LOCKED_VAL))
                break;
        }
    }
}
```





------

**Q7: 什么是读写锁的“饥饿”问题？如何设计避免线程饥饿的公平读写锁？**

**饥饿问题**：

- **读饥饿**：
  - 若读操作频繁，写操作的优先级低，可能导致写线程长期得不到执行。
- **写饥饿**：
  - 若写操作优先，可能导致读线程“饿死”，尤其在高并发读取场景下。



**避免饥饿的公平读写锁设计**：

- **写优先策略**:
  - 控制写优先级，将写操作插入队列的前端。
  - 具体实现：允许当前完成的锁释放后写优先运行（通过队列和调度实现）。
- **使用单队列管理所有请求**:
  - 通过 FIFO 队列（如 Java 的 `ReentrantReadWriteLock`）跟踪每个线程的锁请求，确保线程按照入队顺序公平获取锁。
- **可调优先级**:
  - 某些锁实现可以动态调整优先级。读线程超过某个数量时降低写优先级。

**操作系统中的实际应用**：

- Linux 文件系统的 **rw_semaphore** 实现读写锁，用于读线程与写线程的公平调度。





------

## 分布式锁一致性、Paxos、Raft

**Q8: 分布式锁的实现中为什么要保证多副本写一致性？如果网络分区这种极端情况发生，如何继续保证锁的一致性？**



**多副本写一致性的意义**：

分布式锁需要保证强一致性，如果多副本状态不一致，可能导致以下问题：

- **锁丢失**：某副本认为锁已释放，导致数据出现竞态条件。
- **双重加锁**：两个线程可能同时获取相同资源的锁，破坏系统一致性。



**一致性模型的选择**：

常用的一致性模型：Etcd、ZooKeeper。

最终一致性：如 Dynamo-based 系统，在特定场景下可能引入不一致风险。



**保证锁的一致性方法**:

- 分布式环境下通常使用 **Paxos** 或 **Raft 共识算法** 选举主节点，实现对写操作强一致性保证。



> Q: 什么是分布式共识？
>
> A: 在一个由多个节点组成的分布式系统中，共识算法需要确保：(1) **一致性**：所有非故障节点最终就某个值达成一致 (2) **终止性**：算法最终能产生结果 (3) **合法性**：输出的值必须是某个节点提议的值

### **Paxos 算法详解**

#### 基本角色: 

```PYTHON
class PaxosRole:
    PROPOSER = 1   # 提议者：发起提案
    ACCEPTOR = 2   # 接受者：投票决定是否接受提案
    LEARNER  = 3   # 学习者：学习被选定的值
```

#### 核心流程：两阶段提交

```SHELL
第一阶段：准备阶段（Prepare Phase）
检查是否有更高优先级的谈判在进行,获取历史信息（之前谈过什么合同）,获得多数派的"参与承诺"
┌─────────┐    ┌─────────┐    ┌─────────┐
│Proposer │    │Acceptor1│    │Acceptor2│
└────┬────┘    └────┬────┘    └────┬────┘
     │  Prepare(n)  │              │
     │─────────────>│              │
     │              │              │
     │  Promise(n,  │              │
     │  prev_n,     │              │
     │  prev_v)     │              │
     │<─────────────│              │
     │              │              │
     │  Prepare(n)  │              │
     │────────────────────────────>│
     │              │              │
     │              │  Promise(n,  │
     │              │  prev_n,     │
     │              │  prev_v)     │
     │<────────────────────────────│
     │              │              │
     └──────────────┴──────────────┘

第二阶段：接受阶段（Accept Phase）,正式签约
     │  Accept(n, v)│              │
     │─────────────>│              │
     │              │              │
     │  Accepted    │              │
     │<─────────────│              │
     │              │              │
     │  Accept(n, v)│              │
     │────────────────────────────>│
     │              │              │
     │              │  Accepted    │
     │<────────────────────────────│
     │              │              │
     └──────────────┴──────────────┘
```



### Raft算法详解

Raft 将共识问题分解为三个子问题：（1）**领导者选举**：选出一个Leader  （2）**日志复制**：Leader复制日志到其他节点 （3）**安全性**：确保系统一致性

Raft 通过五个规则保证安全性：

* **选举限制**：只有拥有最新日志的节点才能成为Leader
* **提交规则**：Leader只能提交当前Term的日志
* **状态机安全**：如果日志条目在某个索引位置被提交，那么所有更高索引的位置都包含相同的条目
* **领导者完整性**：选举出的Leader包含所有已提交的日志
* **只追加**：Leader从不删除或覆盖日志



**网络分区时的一致性方案**：

**脑裂问题**：在分布式锁网络分区时，不同分区可能产生多个活动主节点（Leader），导致多个线程之间的锁失效。

**解决方案**：

- **ZooKeeper**：利用会话超时机制，主节点无法发送心跳时自动降级为从节点。
- **Redis Redlock 分布式锁**：
  - Redlock 需要在大多数节点（如 3/5 个副本）获得锁才能认为锁定成功。
  - 超时策略允许锁自动释放，防止死锁。
- Quorum 模型：保持多数派仲裁，无多数派的网络分区会停止服务以避免冲突。



------

## MCS锁和CLH锁

**Q9: 请比较 MCS 锁和 CLH 锁，分别适用于什么场景？操作系统中的应用是什么？**

- **MCS 锁**：
  - 名称来源于 Mellor-Crummey 和 Scott。
  - 它是基于队列的自旋锁，每个线程会在本地维护一个节点，轮询等待前一个节点释放锁，从而减少总线和缓存冲突。
- **CLH 锁**：
  - 名称来源于 Craig、Landin 和 Hagersten。
  - 基于链表的自旋锁，每个线程轮询其“前驱节点”的状态来判断是否可以获取锁。

| 特性               | MCS 锁                   | CLH 锁         |
| ------------------ | ------------------------ | -------------- |
| 队列结构           | 显式队列节点             | 隐式队列       |
| 锁状态存储         | 本地变量                 | 前驱节点变量   |
| CPU 缓存一致性优化 | 是                       | 否             |
| 高并发支持         | 高                       | 高             |
| 空间复杂度         | O(n)（每个线程独立变量） | O(1)（节点少） |

**适用场景**：

- **MCS 锁**：
  - 适用于多核高并发竞争场景，因其减少了缓存一致性流量，避免了伪共享。
- **CLH 锁**：
  - 用于锁竞争较低、线程数量备份较少的场景。
  - 

**操作系统中的应用**：

- **Linux Ticket Locks**：
  - 在 Linux 自旋锁中，类似于 MCS 锁的实现，用于高性能场景。
- **Windows NR 中断锁机制**：
  - MS Windows 的内核可基于 CLH 锁优化线程调度和资源共享。



------

## RDMA分布式锁

**Q10: 在 RDMA 环境中，如何实现高效的分布式锁？能够简述其实现原理和挑战吗？**

- **RDMA 特性**：
  - RDMA 允许节点直接访问远程内存，而无需操作系统介入（如上下文切换、系统调用中断等），提供极低延迟和高带宽的通信能力。
  - 在 RDMA 下，可以通过以下方式实现分布式锁：
    - **基于 CAS（Compare-And-Swap）原语**：利用远程节点内存的原子操作实现锁竞争。
    - **利用一侧加锁、另一侧轮询**：通过远程内存（像状态标志）控制多方的锁获取。

**常见实现模式**:

1. **Centralized Lock Server（集中式锁服务）**：
   - 由一个锁服务器维护锁状态，所有客户端通过 RDMA 读取和更新锁。
   - 实现：
     - 锁状态存储在锁服务器的内存中。
     - 客户端通过 RDMA `COMPARE_AND_WRITE` 操作远程修改锁状态。
   - 优点：
     - 逻辑简单。
     - 高性能（RDMA 的低延迟）。
   - 缺点：
     - 单点瓶颈（可能导致锁服务器负载过大）。
2. **Distributed Ownership（分布式所有权）**：
   - 将锁分布在多台机器上，每个锁由单个节点初始化。
   - 实现：
     - 每台服务器维护一部分锁状态。
     - 客户端通过 RDMA 请求对应锁所在节点尝试锁获取。
   - 优点：
     - 锁状态分散，减少了集中瓶颈。
   - 缺点：
     - 锁的分布管理增加了复杂性。



**RDMA 实现分布式锁的挑战**:

- **一致性保证（Consistency Guarantee）**：
  - 由于远程内存不能直接执行复杂逻辑，锁状态的原子更新需要特别注意。
  - 可通过硬件支持的原子操作（e.g. `COMPARE_AND_SWAP`）解决。
- **网络分区**：
  - 若某节点故障或者网络分区，如何处理锁状态（比如锁丢失或锁悬挂问题）是一个挑战。
  - 解决方法：结合分布式共识算法（如 Raft）处理故障恢复。
- **资源管理瓶颈**：
  - RDMA 的 `QP（Queue Pair）` 数量有限（每对节点间 RDMA 连接需要一个 QP）。





------

## RDMA锁的比较

**Q11: 请描述两种基于 RDMA 的锁实现：基于原子性操作的（e.g., CAS）锁与基于队列锁的分布式锁各自的优缺点？**

在 RDMA 中，分布式锁实现主要分为两类：**基于原子操作（CAS-based Lock）\**和\**基于队列锁（Queue-based Lock）**。

**基于原子操作的锁（CAS-based Lock）**:

- **实现原理**:

  - 使用 RDMA 提供的原子性操作（如 **Compare-And-Swap** 或 **Fetch-And-Add**），让客户端直接尝试修改远程锁状态。

  - `lock` 请求示意伪代码：

  - ```C
    // 伪代码
    remote_mem = lock_server_memory;
    while (CAS(remote_mem, UNLOCKED, LOCKED) == false) {
        sleep();
    }
    // 获取锁逻辑
    ```

  - 锁状态通常存储在远程节点的共享内存中（例如单独的 `uint64_t lock_flag`）。

- **优点**:
  1. 简单高效：直接通过硬件原语完成加锁。
  2. 低延迟：RDMA 的硬件特性消除了操作系统中断和网络栈的延迟。
- **缺点**:
  1. **高冲突场景性能下降**：当多个客户端争抢同一个锁时，可能导致网络风暴（大量 CAS 操作请求）。
  2. **饥饿问题**：由于没有队列管理，某些进程可能永远得不到锁。



**基于队列的锁（Queue-based Lock）**

**实现原理**:

- 基于 RDMA 每个连接的队列特性，构造每个客户端间的多个队列节点。
- 锁争用按照 FIFO 顺序排队，只有队列头可执行锁操作，类似于 MCS/CLH 锁。

```C
enqueue_on_remote(queue_addr);
wait_until_head(queue); // 等待到自己是队头
acquire_lock();
```

- **优点**:
  1. **消除饥饿**：基于队列的调度公平性，确保所有请求都能被服务。
  2. **网络吞吐高效**：避免了高频冲突场景下 CAS 请求的性能下降。
- **缺点**:
  1. 实现复杂：需要管理多个队列节点。
  2. 延迟可能较高：由于排队机制，锁分发速度比 CAS 略慢。



------

## RDMA ABA

Q12: 在 RDMA 中如何避免锁等待队列的翻转问题（如 ABA 问题）？实际应用中有哪些解决方案？

**锁队列翻转问题（ABA 问题）**:

- ABA 问题描述：
  - 线程 A 取锁时，发现锁状态从 “X -> Y -> X” 变化，看似没有变化，但实际上锁状态已被另一个线程改变。
- 在 RDMA 中，由于队列和锁操作基于远程内存，ABA 问题可能导致线程轮询锁状态时产生错误锁定。

**常见解决方案**:

1. **加版本号解决 ABA 问题**:

   - 在锁变量中引入版本号。

   - 例如，锁存储为 64 位，低 32 位存储锁标志，高 32 位存储版本号：

     - ```C
       uint64_t lock_word = (version << 32) | flag;
       ```

       修改操作（CAS）时，必须同时更新版本号。

   - **基于乐观锁和 CAS 双重检查**:

     - 在获取锁之前，线程先读取一次检查锁状态，并在执行操作时再次验证状态是否一致。
     - 这种方式结合了乐观锁和 CAS 的优点。

   - **使用内存屏障提高一致性**:

     - 在锁状态更新之后，确保发布内存屏障（Memory Barrier），阻止重排序错误。

   - > **使用分布式事务或 Raft 算法**: 如果锁的状态较复杂，可以整合分布式事务或共识算法（如 Raft）更新队列状态。



------

## RDMA分布式锁设计因素

**Q13: 假设你需要在 RDMA 环境下实现一个高并发的分布式锁服务，你会如何衡量性能？有哪些关键指标？**

- **加锁延迟（Lock Latency）**：
  - 衡量从发起加锁请求到成功获取锁的总时间。
- **吞吐量（Lock Throughput）**：
  - 一个时间段内系统能够处理的锁操作总数。
- **冲突率（Contention Rate）**：
  - 测量并发锁请求之间的冲突比例（如 CAS 失败或队列等待）。
- **RDMA 消息大小**：
  - 包括锁请求消息和返回成功/失败响应的字节大小，消息小会减少网络开销。
- **公平性（Fairness）**：
  - 衡量锁是否存在饥饿问题（FIFO 或优先级处理是否公平）。